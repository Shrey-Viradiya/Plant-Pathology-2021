{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_TransferLearning_PyTorch_NVIDIA_DALI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvaMbMSWxupU"
      },
      "source": [
        "# Classification using Transfer Learning using PyTorch & NVIDIA DALI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlYmvMBqx44D"
      },
      "source": [
        "## Pneumonia Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sufGWo2bx8oL"
      },
      "source": [
        "### Check GPU Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kkd0vOkxb4f",
        "outputId": "0b2ec3b4-7286-4f82-c4ed-1d911c472b85"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 15 03:47:01 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0    32W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D41IM8Y0yzhh"
      },
      "source": [
        "### Installing NVIDIA-DALI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giP8x9eOyyeE",
        "outputId": "65995545-9c3e-4096-c22f-d8489adae32a"
      },
      "source": [
        "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist nvidia-dali-cuda110"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist\n",
            "Requirement already satisfied: nvidia-dali-cuda110 in /usr/local/lib/python3.7/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b__mKhByDxW"
      },
      "source": [
        "### Downloading Data From Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2gJhcJ8yL8r"
      },
      "source": [
        "- Download Kaggle API key from your Kaggle Account. Go to www.kaggle.com -> My Account -> Create New API token\n",
        "- Place the file in the working directory and execute next cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY7ZgFHpyLdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8df0642-c85b-4a66-b251-35ecc8dc965a"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! mv kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "mv: cannot stat 'kaggle.json': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMf0m3gdMWJ2"
      },
      "source": [
        "### Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMLzbuomMZXf"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import platform\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nvidia.dali import pipeline_def, Pipeline\n",
        "import nvidia.dali.fn as fn\n",
        "import nvidia.dali.types as types\n",
        "from nvidia.dali.plugin.pytorch import DALIGenericIterator, DALIClassificationIterator\n",
        "\n",
        "import torchvision.utils as utils\n",
        "import torch\n",
        "import torchvision\n",
        "import skimage.transform"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2ReWLBtzsj0"
      },
      "source": [
        "### Center cropping and Placing Data in Correct Place"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSWcNX8bzoiV"
      },
      "source": [
        "def image_center_crop(path):\n",
        "    \"\"\"\n",
        "    Makes a square center crop of an img, which is a [h, w, 3] numpy array.\n",
        "    Returns [min(h, w), min(h, w), 3] output with same width and height.\n",
        "    For cropping use numpy slicing.\n",
        "    \"\"\"\n",
        "\n",
        "    image = Image.open(path)\n",
        "    img = np.asarray(image)\n",
        "\n",
        "    shape = img.shape\n",
        "    dim = len(shape)\n",
        "\n",
        "    if dim == 3:\n",
        "        h, w, c = shape\n",
        "    else:\n",
        "        h, w = shape\n",
        "\n",
        "    h_crop = min(h, w)\n",
        "\n",
        "    if dim == 3:\n",
        "        cropped_img = img[\n",
        "            ...,\n",
        "            (h // 2 - h_crop // 2) : (h // 2 + h_crop // 2),\n",
        "            (w // 2 - h_crop // 2) : (w // 2 + h_crop // 2),\n",
        "            :,\n",
        "        ]\n",
        "        image = Image.fromarray(cropped_img, \"RGB\")\n",
        "    else:\n",
        "        cropped_img = img[\n",
        "            ...,\n",
        "            (h // 2 - h_crop // 2) : (h // 2 + h_crop // 2),\n",
        "            (w // 2 - h_crop // 2) : (w // 2 + h_crop // 2),\n",
        "        ]\n",
        "        image = Image.fromarray(cropped_img)\n",
        "\n",
        "    image.save(path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsN8JnNxyY-5",
        "outputId": "f195c0bf-e6dc-4f74-b9c6-9aad3d89472e"
      },
      "source": [
        "if not os.path.exists(\"./data\"):\n",
        "    os.mkdir(\"./data\")\n",
        "\n",
        "if not os.path.exists(\"./data/coronahack-chest-xraydataset.zip\"):\n",
        "    os.chdir(\"./data\")\n",
        "    os.system(\"kaggle datasets download -d praveengovi/coronahack-chest-xraydataset\")\n",
        "    os.chdir(\"..\")\n",
        "else:\n",
        "    print(\"Download found...\")\n",
        "\n",
        "print(\"Starting Extraction\")\n",
        "print(platform.system())\n",
        "if platform.system() in [\"Linux\", \"Darwin\"]:\n",
        "    os.system('unzip -q \"./data/coronahack-chest-xraydataset.zip\" -d \"./data/\"')\n",
        "else:\n",
        "    os.system(\n",
        "        'tar -xf \"./data/coronahack-chest-xraydataset.zip\" --directory \"./data/\" '\n",
        "    )\n",
        "print(\"Extraction Complete\")\n",
        "\n",
        "print(\"Reading Metadata\")\n",
        "images_data = pd.read_csv(\"./data/Chest_xray_Corona_Metadata.csv\")\n",
        "\n",
        "os.mkdir(\"./data/Corona_Classification_data\")\n",
        "os.mkdir(\"./data/Corona_Classification_data/train\")\n",
        "os.mkdir(\"./data/Corona_Classification_data/train/INFECTED\")\n",
        "os.mkdir(\"./data/Corona_Classification_data/train/NORMAL\")\n",
        "os.mkdir(\"./data/Corona_Classification_data/test\")\n",
        "os.mkdir(\"./data/Corona_Classification_data/test/NORMAL\")\n",
        "os.mkdir(\"./data/Corona_Classification_data/test/INFECTED\")\n",
        "\n",
        "\n",
        "print(\"Starting Preprocessing and Moving According to Labels...\")\n",
        "\n",
        "for index, row in images_data.iterrows():\n",
        "    if row[\"Dataset_type\"] == \"TRAIN\":\n",
        "        path_of_image = f\"./data/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset/train/{row['X_ray_image_name']}\"\n",
        "        image_center_crop(path_of_image)\n",
        "        if row[\"Label\"] == \"Normal\":\n",
        "            shutil.move(\n",
        "                path_of_image,\n",
        "                f\"./data/Corona_Classification_data/train/NORMAL/{row['X_ray_image_name']}\",\n",
        "            )\n",
        "\n",
        "        if row[\"Label\"] == \"Pnemonia\":\n",
        "            shutil.move(\n",
        "                path_of_image,\n",
        "                f\"./data/Corona_Classification_data/train/INFECTED/{row['X_ray_image_name']}\",\n",
        "            )\n",
        "\n",
        "    if row[\"Dataset_type\"] == \"TEST\":\n",
        "        path_of_image = f\"./data/Coronahack-Chest-XRay-Dataset/Coronahack-Chest-XRay-Dataset/test/{row['X_ray_image_name']}\"\n",
        "        image_center_crop(path_of_image)\n",
        "        if row[\"Label\"] == \"Normal\":\n",
        "            shutil.move(\n",
        "                path_of_image,\n",
        "                f\"./data/Corona_Classification_data/test/NORMAL/{row['X_ray_image_name']}\",\n",
        "            )\n",
        "\n",
        "        if row[\"Label\"] == \"Pnemonia\":\n",
        "            shutil.move(\n",
        "                path_of_image,\n",
        "                f\"./data/Corona_Classification_data/test/INFECTED/{row['X_ray_image_name']}\",\n",
        "            )\n",
        "    sys.stdout.write(f\"\\rCropping Successfull for {row['X_ray_image_name']}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "print(\"Moving Complete\")\n",
        "shutil.rmtree(\"./data/Coronahack-Chest-XRay-Dataset\")\n",
        "\n",
        "files_to_be_deleted = [\n",
        "    \"1-s2.0-S1684118220300682-main.pdf-002-a1.png\",\n",
        "    \"1-s2.0-S1684118220300682-main.pdf-002-a2.png\",\n",
        "    \"1-s2.0-S1684118220300682-main.pdf-003-b1.png\",\n",
        "    \"1-s2.0-S1684118220300682-main.pdf-003-b2.png\",\n",
        "    \"7EF28E12-F628-4BEC-A8C5-E6277C2E4F60.png\",\n",
        "    \"23E99E2E-447C-46E5-8EB2-D35D12473C39.png\",\n",
        "    \"41591_2020_819_Fig1_HTML.webp-day5.png\",\n",
        "    \"41591_2020_819_Fig1_HTML.webp-day10.png\",\n",
        "]\n",
        "\n",
        "for filename in files_to_be_deleted:\n",
        "    os.remove(f\"./data/Corona_Classification_data/train/INFECTED/{filename}\")\n",
        "print(\"Cleaning Complete\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Extraction\n",
            "Linux\n",
            "Extraction Complete\n",
            "Reading Metadata\n",
            "Starting Preprocessing and Moving According to Labels...\n",
            "Cropping Successfull for person1632_virus_2827.jpegMoving Complete\n",
            "Cleaning Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi2hygjF00Gl"
      },
      "source": [
        "### Creating NVIDIA DALI PipeLine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jry3GWAzzhR"
      },
      "source": [
        "@pipeline_def\n",
        "def HybridPipelineTrain(num_gpus, root_path, output_size):\n",
        "    device_id = Pipeline.current().device_id\n",
        "    jpegs, labels = fn.readers.file(\n",
        "        name='Reader', file_root =root_path, random_shuffle=True, shard_id=device_id, num_shards=num_gpus\n",
        "        )\n",
        "    images = fn.decoders.image(jpegs, device='mixed')\n",
        "    images = fn.flip(images, horizontal = fn.random.coin_flip(), vertical = fn.random.coin_flip(), device = \"gpu\")\n",
        "    images = fn.rotate(images, angle=fn.random.uniform(range=(-90., 90.)), device = \"gpu\")\n",
        "    images = fn.random_resized_crop(images,\n",
        "            size = output_size,\n",
        "            random_area = [0.4, 1.0],\n",
        "            random_aspect_ratio = [0.5, 1.5],\n",
        "            device=\"gpu\",\n",
        "            )\n",
        "    images = fn.crop_mirror_normalize(\n",
        "            images,\n",
        "            dtype=types.FLOAT,\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std=[255., 255., 255.],\n",
        "            )\n",
        "\n",
        "    return images, labels"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEgxSiwEO0nz"
      },
      "source": [
        "@pipeline_def\n",
        "def HybridPipelineTest(num_gpus, root_path, output_size):\n",
        "    device_id = Pipeline.current().device_id\n",
        "    jpegs, labels = fn.readers.file(\n",
        "        name='Reader', file_root =root_path, random_shuffle=True, shard_id=device_id, num_shards=num_gpus\n",
        "        )\n",
        "    images = fn.decoders.image(jpegs, device='mixed')\n",
        "    images = fn.resize(images, size=output_size)\n",
        "    images = fn.crop_mirror_normalize(\n",
        "            images,\n",
        "            dtype=types.FLOAT,\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std=[255., 255., 255.],\n",
        "            )\n",
        "    return images, labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASCklU4z53zs"
      },
      "source": [
        "### Checking PipeLine Validity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gSY_Zxw9_nU"
      },
      "source": [
        "!mkdir Augmentation_Check"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4xSn_cG4CaA",
        "outputId": "04e28e76-ba22-4660-f239-30bfd3856d77"
      },
      "source": [
        "label_range = (0, 1)\n",
        "pipes = [HybridPipelineTrain(batch_size=64, num_threads=2, device_id=device_id, num_gpus=1, \n",
        "                             root_path=\"/content/data/Corona_Classification_data/train\",\n",
        "                             output_size = (224,224)\n",
        "                             ) for device_id in range(1)]\n",
        "\n",
        "for pipe in pipes:\n",
        "    pipe.build()\n",
        "\n",
        "dali_iter = DALIGenericIterator(pipes, ['data', 'label'], reader_name='Reader')\n",
        "\n",
        "count = 0\n",
        "for i, data in enumerate(dali_iter):\n",
        "    # Testing correctness of labels\n",
        "    for d in data:\n",
        "        label = d[\"label\"]\n",
        "        image = d[\"data\"]\n",
        "        grid = utils.make_grid(image)\n",
        "        plt.imshow(grid.cpu().numpy().transpose((1, 2, 0)))\n",
        "        plt.savefig(f\"/content/Augmentation_Check/{count}.png\")\n",
        "        plt.close()\n",
        "        count+=1\n",
        "        ## labels need to be integers\n",
        "        assert(np.equal(np.mod(label, 1), 0).all())\n",
        "        ## labels need to be in range pipe_name[2]\n",
        "        assert((label >= label_range[0]).all())\n",
        "        assert((label <= label_range[1]).all())\n",
        "\n",
        "dali_iter.reset()\n",
        "print(\"OK\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKDlplXOMPyq"
      },
      "source": [
        "### Model and Training Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdenPSl1BKJs"
      },
      "source": [
        "# Dictionary for pretrained models and their last layer name\n",
        "pretrained_models = {\n",
        "    \"ResNet18\": [torchvision.models.resnet18, \"layer4\", (224, 224)],\n",
        "    \"ResNet34\": [torchvision.models.resnet34, \"layer4\", (224, 224)],\n",
        "    \"ResNet50\": [torchvision.models.resnet50, \"layer4\", (224, 224)],\n",
        "    \"ResNet101\": [torchvision.models.resnet101, \"layer4\", (224, 224)],\n",
        "    \"ResNet152\": [torchvision.models.resnet152, \"layer4\", (224, 224)],\n",
        "    \"Alexnet\": [torchvision.models.alexnet, \"features\", (256, 256)],\n",
        "    \"VGG11\": [torchvision.models.vgg11_bn, \"features\", (224, 224)],\n",
        "    \"VGG13\": [torchvision.models.vgg13_bn, \"features\", (224, 224)],\n",
        "    \"VGG16\": [torchvision.models.vgg16_bn, \"features\", (224, 224)],\n",
        "    \"VGG19\": [torchvision.models.vgg19_bn, \"features\", (224, 224)],\n",
        "    \"GoogleNet\": [torchvision.models.googlenet, \"inception5b\", (224, 224)],\n",
        "    \"Inception\": [torchvision.models.inception_v3, \"Mixed_7c\", (299, 299)],\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n59JPTT5M1Yx"
      },
      "source": [
        "# The main model object\n",
        "class PneumoniaDetection:\n",
        "    \"\"\"\n",
        "    Model Architecture and Forward Training Path for the Pneumonia Detection\n",
        "    Idea is to use transfer Learning\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model=\"ResNet18\", colab=False):\n",
        "        assert base_model in [\n",
        "            \"ResNet18\",\n",
        "            \"ResNet34\",\n",
        "            \"ResNet50\",\n",
        "            \"ResNet101\",\n",
        "            \"ResNet152\",\n",
        "            \"Alexnet\",\n",
        "            \"VGG11\",\n",
        "            \"VGG13\",\n",
        "            \"VGG16\",\n",
        "            \"VGG19\",\n",
        "            \"GoogleNet\",\n",
        "            \"Inception\",\n",
        "        ]\n",
        "\n",
        "        # saving base model name to use it in saving the model\n",
        "        self.base_model = base_model\n",
        "        if colab:\n",
        "            self.colab_training = f\"drive/My Drive/{self.base_model}\"\n",
        "        else:\n",
        "            self.colab_training = \".\"\n",
        "\n",
        "        if not os.path.exists(f\"{self.colab_training}/model\"):\n",
        "            os.mkdir(f\"{self.colab_training}/model\")\n",
        "        if not os.path.exists(f\"{self.colab_training}/model_results\"):\n",
        "            os.mkdir(f\"{self.colab_training}/model_results\")\n",
        "\n",
        "        if os.path.exists(f\"{self.colab_training}/model/ConvModel_{self.base_model}\"):\n",
        "            # check if the model is intialized before\n",
        "            self.model = torch.load(\n",
        "                f\"{self.colab_training}/model/ConvModel_{self.base_model}\"\n",
        "            )\n",
        "        else:\n",
        "            # If not initialized before\n",
        "            # Download it and save it\n",
        "            self.model = pretrained_models[self.base_model][0](pretrained=True)\n",
        "            for name, param in self.model.named_parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "            # Modify last Fully Connected layer to predict for\n",
        "            # Our requirements\n",
        "            if self.base_model in [\"Alexnet\", \"VGG11\", \"VGG13\", \"VGG16\", \"VGG19\"]:\n",
        "                num_ftrs = self.model.classifier[6].in_features\n",
        "                self.model.classifier[6] = torch.nn.Linear(num_ftrs, 2)\n",
        "            else:\n",
        "                self.model.fc = torch.nn.Sequential(\n",
        "                    torch.nn.Linear(self.model.fc.in_features, 500),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Dropout(),\n",
        "                    torch.nn.Linear(500, 2),\n",
        "                )\n",
        "\n",
        "            # Save model\n",
        "            torch.save(\n",
        "                self.model, f\"{self.colab_training}/model/ConvModel_{self.base_model}\"\n",
        "            )\n",
        "\n",
        "        # get final model for using it in Class Activation Map\n",
        "        self.final_layer = self.model._modules.get(\n",
        "            pretrained_models[self.base_model][1]\n",
        "        )\n",
        "\n",
        "        # Different image transformations for training, testing and displaying\n",
        "        self.train_transformation = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.RandomHorizontalFlip(),\n",
        "                torchvision.transforms.RandomVerticalFlip(),\n",
        "                torchvision.transforms.RandomRotation(25),\n",
        "                torchvision.transforms.RandomResizedCrop(\n",
        "                    pretrained_models[self.base_model][2],\n",
        "                    scale=(0.4, 1.0),\n",
        "                    ratio=(0.5, 1.5),\n",
        "                    interpolation=2,\n",
        "                ),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize(\n",
        "                    mean=[0,0,0], std=[255,255,255]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        self.test_transformation = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.Resize(pretrained_models[self.base_model][2]),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize(\n",
        "                    mean=[0,0,0], std=[255,255,255]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        self.display_transformation = torchvision.transforms.Compose(\n",
        "            [\n",
        "                torchvision.transforms.Resize(pretrained_models[self.base_model][2]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        optimizer,\n",
        "        loss_fun,\n",
        "        train_data,\n",
        "        test_data,\n",
        "        epochs=20,\n",
        "        early_stopping_threshold=4,\n",
        "        device=\"cuda\",\n",
        "        dali = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Train function:\n",
        "        parameters:\n",
        "        optimizer   : optimizer object\n",
        "        loss_fun    : Loss Function object\n",
        "        train_data  : train dataloader\n",
        "        test_data   : test  dataloader\n",
        "        epochs      : default value 20\n",
        "        early_stopping_threshold : Early stopping threshold\n",
        "        device      : 'cuda' or 'cpu', default 'cuda'\n",
        "        \"\"\"\n",
        "\n",
        "        # transfer model to device available\n",
        "        self.model.to(device)\n",
        "\n",
        "        max_accurracy = 0.0\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "        train_accuracies = []\n",
        "        test_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start = time.time()\n",
        "\n",
        "            training_loss = 0.0\n",
        "\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            # Training over batches\n",
        "            self.model.train(mode=True)\n",
        "            for i, train_batch in enumerate(train_data):\n",
        "                if dali:\n",
        "                    d = train_batch[0]\n",
        "                    train_images, train_labels = d[\"data\"], d[\"label\"]\n",
        "                    # train_images = train_images.permute(0,3,1,2)\n",
        "                    train_images = train_images.to(device)\n",
        "                    train_labels = train_labels.to(device, dtype=torch.int64).squeeze()\n",
        "                else:\n",
        "                    train_images, train_labels = train_batch\n",
        "                    train_images = train_images.to(device)\n",
        "                    train_labels = train_labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                train_output = self.model(train_images)\n",
        "\n",
        "                if self.base_model == \"Inception\":\n",
        "                    train_output = train_output.logits\n",
        "\n",
        "                train_loss = loss_fun(train_output, train_labels)\n",
        "\n",
        "                train_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                training_loss += train_loss.item()\n",
        "\n",
        "                _, train_predicted = torch.max(train_output.data, 1)\n",
        "\n",
        "                train_total += train_labels.size(0)\n",
        "                train_ccount = (train_predicted == train_labels).sum().item()\n",
        "                train_correct += train_ccount\n",
        "                \n",
        "                text = !nvidia-smi\n",
        "                text = text[9][60:65]\n",
        "                \n",
        "                sys.stdout.write(\n",
        "                    f\"\\rEpoch {epoch+1:03d}\\t\"\n",
        "                    f\"Train Loss => {train_loss:08.4f} \"\n",
        "                    f\"Train Accuracy => \"\n",
        "                    f\"{train_ccount/train_images.shape[0]*100:06.2f} \"\n",
        "                    f\"GPU Utilization => {text}\"\n",
        "                )\n",
        "            \n",
        "            if dali:\n",
        "                train_data.reset()\n",
        "\n",
        "            training_accuracy = train_correct / train_total * 100\n",
        "\n",
        "            valid_loss = 0.0\n",
        "            test_correct = 0\n",
        "            test_total = 0\n",
        "            misses = 0\n",
        "            previous_accuracy = 0\n",
        "            tp = 0\n",
        "            fp = 0\n",
        "            fn = 0\n",
        "\n",
        "            # Test over batches\n",
        "            self.model.train(mode=False)\n",
        "            with torch.no_grad():\n",
        "                for i, test_batch in enumerate(test_data):\n",
        "                    if dali:\n",
        "                        d = test_batch[0]\n",
        "                        test_images, test_labels = d[\"data\"], d[\"label\"]\n",
        "                        # test_images = test_images.permute(0,3,1,2)\n",
        "                        test_images = test_images.to(device)\n",
        "                        test_labels = test_labels.to(device, dtype=torch.int64).squeeze()\n",
        "                    else:\n",
        "                        test_images, test_labels = test_batch\n",
        "                        test_images = test_images.to(device)\n",
        "                        test_labels = test_labels.to(device)\n",
        "\n",
        "                    test_output = self.model(test_images)\n",
        "\n",
        "                    test_loss = loss_fun(test_output, test_labels)\n",
        "                    valid_loss += test_loss.item()\n",
        "\n",
        "                    _, test_predicted = torch.max(test_output.data, 1)\n",
        "\n",
        "                    test_total += test_labels.size(0)\n",
        "                    test_ccount = (test_predicted == test_labels).sum().item()\n",
        "                    tp += ((test_labels == 1) & ((test_predicted) == 1)).sum().item()\n",
        "                    fn += ((test_labels != 0) & ((test_predicted) == 0)).sum().item()\n",
        "                    fp += ((test_labels != 1) & ((test_predicted) == 1)).sum().item()\n",
        "                    test_correct += test_ccount\n",
        "\n",
        "            testing_accuracy = test_correct / test_total * 100\n",
        "            testing_precision = tp / (tp + fp) * 100\n",
        "            testing_recall = tp / (tp + fn) * 100\n",
        "            testing_f1 = (\n",
        "                2.0\n",
        "                * testing_recall\n",
        "                * testing_precision\n",
        "                / (testing_recall + testing_precision)\n",
        "            )\n",
        "            if dali:\n",
        "                test_data.reset()\n",
        "\n",
        "            sys.stdout.flush()\n",
        "            sys.stdout.write(\"\\r\")\n",
        "\n",
        "            time_taken = time.time() - start\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch + 1:03d}\\n\"\n",
        "                f\"Train Loss => {training_loss:08.4f} \"\n",
        "                f\"Train Accuracy => {training_accuracy:06.2f} \"\n",
        "                f\"Test Loss => {valid_loss:08.4f} \"\n",
        "                f\"Test Accuracy => {testing_accuracy:06.2f} \"\n",
        "                f\"Test Precision => {testing_precision:06.2f}\\n\"\n",
        "                f\"Test Recall => {testing_recall:06.2f} \"\n",
        "                f\"Test F1 Score => {testing_f1:06.2f} \"\n",
        "                f\"Time Taken => {time_taken:08.4f}\"\n",
        "                f\"\\n{'='*150}\"\n",
        "            )\n",
        "\n",
        "            train_losses.append(training_loss)\n",
        "            test_losses.append(valid_loss)\n",
        "            train_accuracies.append(training_accuracy)\n",
        "            test_accuracies.append(testing_accuracy)\n",
        "\n",
        "            # Save if it is better model than max_accuracy\n",
        "            if testing_accuracy > max_accurracy:\n",
        "                max_accurracy = testing_accuracy\n",
        "                torch.save(\n",
        "                    self.model,\n",
        "                    f\"{self.colab_training}/model/ConvModel_{self.base_model}\",\n",
        "                )\n",
        "\n",
        "                with open(\n",
        "                    f\"{self.colab_training}/model_results/ConvModel_{self.base_model}_results.txt\",\n",
        "                    \"w\",\n",
        "                ) as f:\n",
        "                    f.writelines(\n",
        "                        [\n",
        "                            f\"BaseModel: {self.base_model}\\n\",\n",
        "                            f\"Epochs: {epoch + 1:03d}\\n\",\n",
        "                            f\"Train Dataloader Batch Size: {train_data.batch_size}\\n\",\n",
        "                            f\"Test Dataloader Batch Size: {test_data.batch_size}\\n\",\n",
        "                            f\"Params for Optimizer: {optimizer.__repr__()}\\n\",\n",
        "                            f\"Train Loss: {training_loss:08.4f}\\n\",\n",
        "                            f\"Test Loss: {valid_loss:08.4f}\\n\",\n",
        "                            f\"Train Accuracy: {training_accuracy:06.2f}\\n\",\n",
        "                            f\"Test Accuracy: {testing_accuracy:06.2f}\\n\",\n",
        "                            f\"Test Precision: {testing_precision:06.2f}\\n\",\n",
        "                            f\"Test Recall: {testing_recall:06.2f}\\n\",\n",
        "                            f\"Test F1 Score: {testing_f1:06.2f}\\n\",\n",
        "                            f\"Time Taken: {time_taken:08.4f} seconds\",\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "            # Decide and stop early if needed\n",
        "            if epoch >= 1:\n",
        "                if (\n",
        "                    previous_accuracy > testing_accuracy\n",
        "                    and misses < early_stopping_threshold\n",
        "                ):\n",
        "                    misses += 1\n",
        "                    previous_accuracy = testing_accuracy\n",
        "                elif previous_accuracy > testing_accuracy:\n",
        "                    print(f\"Early Stopping....\")\n",
        "                    print(\n",
        "                        f\"Epoch {epoch + 1:03d}\\t\"\n",
        "                        f\"Train Loss => {training_loss:08.4f} \"\n",
        "                        f\"Train accuracy => {training_accuracy:06.2f} \"\n",
        "                        f\"Test Loss => {valid_loss:08.4f} \"\n",
        "                        f\"Test Accuracy => {testing_accuracy:06.2f} \"\n",
        "                        f\"Test Precision => {testing_precision:06.2f} \"\n",
        "                        f\"Test Recall => {testing_recall:06.2f} \"\n",
        "                        f\"Test F1 Score => {testing_f1:06.2f} \"\n",
        "                        f\"Time Taken => {time_taken:08.4f}\"\n",
        "                    )\n",
        "                    break\n",
        "            previous_accuracy = testing_accuracy\n",
        "\n",
        "            np.save(\n",
        "                f\"{self.colab_training}/model/train_losses_{self.base_model}\",\n",
        "                train_losses,\n",
        "            )\n",
        "            np.save(\n",
        "                f\"{self.colab_training}/model/train_accuracies_{self.base_model}\",\n",
        "                train_accuracies,\n",
        "            )\n",
        "            np.save(\n",
        "                f\"{self.colab_training}/model/test_losses_{self.base_model}\",\n",
        "                test_losses,\n",
        "            )\n",
        "            np.save(\n",
        "                f\"{self.colab_training}/model/test_accuracies_{self.base_model}\",\n",
        "                test_accuracies,\n",
        "            )\n",
        "\n",
        "    def test(self, loss_fun, test_data, device=\"cuda\", has_labels=False, dali=False):\n",
        "        print(\"Starting Evaluating....\")\n",
        "        start = time.time()\n",
        "        self.model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        tp = 0\n",
        "        fp = 0\n",
        "        fn = 0\n",
        "        predictions = []\n",
        "\n",
        "        # Without changing parameters\n",
        "        with torch.no_grad():\n",
        "            # Testing over batches\n",
        "            for i, batch in enumerate(test_data):\n",
        "                if dali:\n",
        "                    d = batch[0]\n",
        "                    test_images, test_labels = d[\"data\"], d[\"label\"]\n",
        "                    # test_images = test_images.permute(0,3,1,2)\n",
        "                    test_images = test_images.to(device)\n",
        "                    if has_labels:\n",
        "                        test_labels = test_labels.to(device, dtype=torch.int64).squeeze()\n",
        "                else:\n",
        "                    test_images, test_labels = batch\n",
        "                    test_images = test_images.to(device)\n",
        "                    if has_labels:\n",
        "                        test_labels = test_labels.to(device)\n",
        "\n",
        "                output = self.model(test_images)\n",
        "                if has_labels:\n",
        "                    loss = loss_fun(output, test_labels)\n",
        "                    test_loss += loss.item()\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                predictions.extend(list(np.asarray(predicted.to(\"cpu\"))))\n",
        "                if has_labels:\n",
        "                    total += test_labels.size(0)\n",
        "                    correct += (predicted == test_labels).sum().item()\n",
        "                    tp += ((test_labels == 1) & ((predicted) == 1)).sum().item()\n",
        "                    fn += ((test_labels != 0) & ((predicted) == 0)).sum().item()\n",
        "                    fp += ((test_labels != 1) & ((predicted) == 1)).sum().item()\n",
        "\n",
        "        if has_labels:\n",
        "            testing_accuracy = correct / total * 100\n",
        "            testing_precision = tp / (tp + fp) * 100\n",
        "            testing_recall = tp / (tp + fn) * 100\n",
        "            testing_f1 = (\n",
        "                2.0\n",
        "                * testing_recall\n",
        "                * testing_precision\n",
        "                / (testing_recall + testing_precision)\n",
        "            )\n",
        "\n",
        "            print(\n",
        "                f\"Test Loss => {test_loss:08.4f} \"\n",
        "                f\"Test accuracy => {testing_accuracy:06.2f} \"\n",
        "                f\"Test Precision => {testing_precision:06.2f} \"\n",
        "                f\"Test Recall => {testing_recall:06.2f} \"\n",
        "                f\"Test F1 Score => {testing_f1:06.2f} \"\n",
        "                f\"Time Taken => {time.time() - start:08.4f}\"\n",
        "            )\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def CAM(self, image_path_input, overlay_path_output, device=\"cuda\"):\n",
        "        \"\"\"\n",
        "        CAM - Class Activation Map\n",
        "        \"\"\"\n",
        "\n",
        "        # open image\n",
        "        image = Image.open(image_path_input)\n",
        "        image = image.convert(\"RGB\")\n",
        "        print(image.mode)\n",
        "\n",
        "        tensor = self.test_transformation(image)\n",
        "\n",
        "        prediction_var = torch.autograd.Variable(\n",
        "            (tensor.unsqueeze(0)).cuda(), requires_grad=True\n",
        "        )\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "        class SaveFeatures:\n",
        "            features = None\n",
        "\n",
        "            def __init__(self, m):\n",
        "                self.hook = m.register_forward_hook(self.hook_fn)\n",
        "\n",
        "            def hook_fn(self, module, input, output):\n",
        "                self.features = ((output.cpu()).data).numpy()\n",
        "\n",
        "            def remove(self):\n",
        "                self.hook.remove()\n",
        "\n",
        "        activated_features = SaveFeatures(self.final_layer)\n",
        "        prediction_var = prediction_var.to(device)\n",
        "        prediction = self.model(prediction_var)\n",
        "\n",
        "        pred_probabilities = torch.nn.functional.softmax(\n",
        "            prediction, dim=0\n",
        "        ).data.squeeze()\n",
        "\n",
        "        activated_features.remove()\n",
        "\n",
        "        torch.topk(pred_probabilities, 1)\n",
        "\n",
        "        def getCAM(feature_conv, weight_fc, class_idx):\n",
        "            _, nc, h, w = feature_conv.shape\n",
        "            cam = weight_fc[class_idx].dot(feature_conv.reshape((nc, h * w)))\n",
        "            cam = cam.reshape(h, w)\n",
        "            cam = cam - np.min(cam)\n",
        "            cam_img = cam / np.max(cam)\n",
        "            return [cam_img]\n",
        "\n",
        "        weight_softmax_params = list(self.model._modules.get(\"fc\").parameters())\n",
        "        weight_softmax = np.squeeze(weight_softmax_params[0].cpu().data.numpy())\n",
        "\n",
        "        class_idx = torch.topk(pred_probabilities, 1)[1].int()\n",
        "\n",
        "        overlay = getCAM(activated_features.features, weight_softmax, class_idx)\n",
        "\n",
        "        plt.figure(figsize=(32, 15))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(self.display_transformation(image))\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(self.display_transformation(image))\n",
        "        plt.imshow(\n",
        "            skimage.transform.resize(overlay[0], tensor.shape[1:3]),\n",
        "            alpha=0.4,\n",
        "            cmap=\"jet\",\n",
        "        )\n",
        "        plt.savefig(overlay_path_output)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC3Lw1CpNoq8",
        "outputId": "f463cdd8-315f-4d3b-ccf8-fa7a3383a810"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"Using GPU\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY0wSz-aN6lQ"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8yfi4VqN6UT"
      },
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.0001\n",
        "dali = True\n",
        "N_GPUS = 1\n",
        "epochs = 10\n",
        "base_model = \"ResNet50\"\n",
        "optimizer_name = 'Adam'\n",
        "colab = False\n",
        "train_data_path = \"./data/Corona_Classification_data/train/\"\n",
        "test_data_path = \"./data/Corona_Classification_data/test/\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gi5xuUqNxS3",
        "outputId": "4106b3d2-7383-4a29-f211-5248ceff3da6"
      },
      "source": [
        "print(\"Creating Model Object: \")\n",
        "model = PneumoniaDetection(base_model, colab=colab)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating Model Object: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:804: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vijRu3VRP1Mj"
      },
      "source": [
        "### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8GQ2qnPN1Cp"
      },
      "source": [
        "if dali:\n",
        "    pipes = [HybridPipelineTrain(batch_size=batch_size, num_threads=2, device_id=device_id, num_gpus=N_GPUS, \n",
        "                             root_path=train_data_path,\n",
        "                             output_size = pretrained_models[base_model][2]\n",
        "                             ) for device_id in range(N_GPUS)]\n",
        "\n",
        "    for pipe in pipes:\n",
        "        pipe.build()\n",
        "\n",
        "    train_data_loader = DALIGenericIterator(pipes, ['data', 'label'], reader_name='Reader')\n",
        "\n",
        "    pipes2 = [HybridPipelineTest(batch_size=batch_size, num_threads=2, device_id=device_id, num_gpus=N_GPUS, \n",
        "                             root_path=test_data_path,\n",
        "                             output_size = pretrained_models[base_model][2]\n",
        "                             ) for device_id in range(N_GPUS)]\n",
        "    for pipe in pipes2:\n",
        "        pipe.build()\n",
        "\n",
        "    test_data_loader = DALIGenericIterator(pipes, ['data', 'label'], reader_name='Reader')\n",
        "else:\n",
        "    print(\"Setting up Data Directories\")\n",
        "    \n",
        "    train_data = torchvision.datasets.ImageFolder(\n",
        "        root=train_data_path, transform=model.train_transformation\n",
        "    )\n",
        "    \n",
        "    test_data = torchvision.datasets.ImageFolder(\n",
        "        root=test_data_path, transform=model.test_transformation\n",
        "    )    \n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "    test_data_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=batch_size, shuffle=True\n",
        "    )"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJnzrKOdP3fm"
      },
      "source": [
        "### Optimizer and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa2LVYgDQUvq"
      },
      "source": [
        "optimizers = {\n",
        "    \"Adam\": torch.optim.Adam,\n",
        "    \"SGD\": torch.optim.SGD,\n",
        "    \"RMSprop\": torch.optim.RMSprop,\n",
        "    \"Adagrad\": torch.optim.Adagrad,\n",
        "    \"Adadelta\": torch.optim.Adadelta,\n",
        "}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SczhRrCQP3Hx",
        "outputId": "dfdc39b9-7172-4087-da53-8c8cce2dcd44"
      },
      "source": [
        "optimizer = optimizers[optimizer_name](\n",
        "    model.model.parameters(), lr=learning_rate\n",
        ")\n",
        "\n",
        "print(\"Starting Training\")    \n",
        "model.train(\n",
        "    optimizer,\n",
        "    torch.nn.CrossEntropyLoss(),\n",
        "    train_data_loader,\n",
        "    test_data_loader,\n",
        "    epochs=epochs,\n",
        "    device=device,\n",
        "    dali=dali\n",
        ")\n",
        "print(\"Completed Training\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training\n",
            "Epoch 001\n",
            "Train Loss => 015.6931 Train Accuracy => 091.51 Test Loss => 031.6230 Test Accuracy => 085.84 Test Precision => 098.39\n",
            "Test Recall => 045.12 Test F1 Score => 061.87 Time Taken => 062.9397\n",
            "======================================================================================================================================================\n",
            "Epoch 002\n",
            "Train Loss => 009.5347 Train Accuracy => 095.62 Test Loss => 009.9258 Test Accuracy => 095.22 Test Precision => 093.84\n",
            "Test Recall => 086.87 Test F1 Score => 090.22 Time Taken => 062.2051\n",
            "======================================================================================================================================================\n",
            "Epoch 003\n",
            "Train Loss => 007.3890 Train Accuracy => 096.59 Test Loss => 007.4554 Test Accuracy => 096.59 Test Precision => 089.86\n",
            "Test Recall => 097.63 Test F1 Score => 093.58 Time Taken => 062.7988\n",
            "======================================================================================================================================================\n",
            "Epoch 004\n",
            "Train Loss => 006.2090 Train Accuracy => 097.24 Test Loss => 017.9958 Test Accuracy => 092.19 Test Precision => 076.95\n",
            "Test Recall => 098.72 Test F1 Score => 086.49 Time Taken => 062.0153\n",
            "======================================================================================================================================================\n",
            "Epoch 005\n",
            "Train Loss => 006.7074 Train Accuracy => 097.10 Test Loss => 005.3144 Test Accuracy => 097.44 Test Precision => 096.57\n",
            "Test Recall => 093.30 Test F1 Score => 094.91 Time Taken => 063.0100\n",
            "======================================================================================================================================================\n",
            "Epoch 006\n",
            "Train Loss => 006.3875 Train Accuracy => 097.26 Test Loss => 006.3688 Test Accuracy => 097.07 Test Precision => 095.17\n",
            "Test Recall => 093.17 Test F1 Score => 094.16 Time Taken => 062.0193\n",
            "======================================================================================================================================================\n",
            "Epoch 007\n",
            "Train Loss => 005.2847 Train Accuracy => 097.63 Test Loss => 004.4557 Test Accuracy => 098.06 Test Precision => 096.35\n",
            "Test Recall => 095.99 Test F1 Score => 096.17 Time Taken => 062.9026\n",
            "======================================================================================================================================================\n",
            "Epoch 008\n",
            "Train Loss => 004.9061 Train Accuracy => 097.87 Test Loss => 007.2901 Test Accuracy => 096.57 Test Precision => 095.22\n",
            "Test Recall => 091.09 Test F1 Score => 093.11 Time Taken => 062.1385\n",
            "======================================================================================================================================================\n",
            "Epoch 009\n",
            "Train Loss => 005.6532 Train Accuracy => 097.55 Test Loss => 004.9151 Test Accuracy => 097.65 Test Precision => 095.83\n",
            "Test Recall => 094.91 Test F1 Score => 095.37 Time Taken => 062.8601\n",
            "======================================================================================================================================================\n",
            "Epoch 010\n",
            "Train Loss => 004.4148 Train Accuracy => 098.02 Test Loss => 005.5839 Test Accuracy => 097.54 Test Precision => 091.83\n",
            "Test Recall => 099.18 Test F1 Score => 095.36 Time Taken => 062.2282\n",
            "======================================================================================================================================================\n",
            "Completed Training\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}